{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "layout: post\n",
    "title: Virtebi Algorithm and Hidden Markov Model - Part 2\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk.corpus as corp\n",
    "import nltk\n",
    "import pomegranate as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the corpus from nltk repository\n",
    "if not corp.brown:\n",
    "    nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown corpus has 1,161,192 tagged words / unigram\n"
     ]
    }
   ],
   "source": [
    "print('Brown corpus has {:,} tagged words / unigram'.format(len(corp.brown.tagged_words())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.brown.tagged_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have sample: 5000 tokens\n"
     ]
    }
   ],
   "source": [
    "# sampling for the first 5000 tokens just for demo sake\n",
    "sample = corp.brown.tagged_words()[:5000]\n",
    "print('We have sample: {} tokens'.format(len(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_discrete_distributions_per_words(tagged_tokens):\n",
    "    tag_probs = dict()\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag not in tag_probs:\n",
    "            tag_probs[tag] = dict()\n",
    "            tag_probs[tag]['count_tag'] = dict()\n",
    "            tag_probs[tag]['occurence'] = 1\n",
    "        else:\n",
    "            tag_probs[tag]['occurence'] += 1\n",
    "            \n",
    "        if token not in tag_probs[tag]['count_tag']:\n",
    "            tag_probs[tag]['count_tag'][token] = 1\n",
    "        else:\n",
    "            tag_probs[tag]['count_tag'][token] += 1\n",
    "            \n",
    "    for tag in tag_probs:\n",
    "        tag_probs[tag]['probs'] = dict()\n",
    "        for token in tag_probs[tag]['count_tag']:\n",
    "            tag_probs[tag]['probs'][token] = float(tag_probs[tag]['count_tag'][token]) / float(tag_probs[tag]['occurence'])\n",
    "    \n",
    "    return tag_probs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_transition_probabilities_per_words(tagged_tokens):\n",
    "    transition_probs = dict()\n",
    "    for i in range(len(tagged_tokens)):\n",
    "        current_tag = tagged_tokens[i][1]\n",
    "        if current_tag not in transition_probs:\n",
    "            transition_probs[current_tag] = dict()\n",
    "            transition_probs[current_tag]['occurence'] = 0\n",
    "            transition_probs[current_tag]['count_transition'] = dict()\n",
    "        \n",
    "        # evaluate previous tag\n",
    "        if i > 0:\n",
    "            previous_tag = tagged_tokens[i-1][1]\n",
    "            pt = tagged_tokens[i-1][0]\n",
    "            transition_probs[previous_tag]['occurence'] += 1\n",
    "            \n",
    "            # special case for <start> tag\n",
    "            if pt == '.':\n",
    "                if '<start>' not in transition_probs:\n",
    "                    transition_probs['<start>'] = dict()\n",
    "                    transition_probs['<start>']['occurence'] = 0\n",
    "                    transition_probs['<start>']['count_transition'] = dict()\n",
    "                if current_tag not in transition_probs['<start>']['count_transition']:\n",
    "                    transition_probs['<start>']['count_transition'][current_tag] = 0\n",
    "                    \n",
    "                transition_probs['<start>']['count_transition'][current_tag] += 1\n",
    "                transition_probs['<start>']['occurence'] += 1\n",
    "                    \n",
    "            \n",
    "            #init\n",
    "            if current_tag not in transition_probs[previous_tag]['count_transition']:\n",
    "                transition_probs[previous_tag]['count_transition'][current_tag] = 0\n",
    "                \n",
    "            transition_probs[previous_tag]['count_transition'][current_tag] += 1\n",
    "    for tag in transition_probs:\n",
    "        transition_probs[tag]['probs'] = dict()\n",
    "        for transit_tag in transition_probs[tag]['count_transition']:\n",
    "            transition_probs[tag]['probs'][transit_tag] = \\\n",
    "                float(transition_probs[tag]['count_transition'][transit_tag]) / float(transition_probs[tag]['occurence'])\n",
    "    \n",
    "    return transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hmm_model(token_dist, transition_dist, model_name='hmm-tagger'):\n",
    "    state_dict = dict()\n",
    "    for token in token_dist:\n",
    "        state_dict[token] = \\\n",
    "            pm.State(\n",
    "                pm.DiscreteDistribution(\n",
    "                    token_dist[token]['probs']\n",
    "                )\n",
    "                , name=token\n",
    "            )\n",
    "            \n",
    "    model = pm.HiddenMarkovModel(model_name)\n",
    "    model.add_states(list(state_dict.values()))\n",
    "    \n",
    "    # initialization for starting tokens\n",
    "    for token, prob in transition_dist['.']['probs'].items():\n",
    "        model.add_transition(state_dict[token], model.end, prob)\n",
    "        \n",
    "    for token, prob in transition_dist['<start>']['probs'].items():\n",
    "        model.add_transition(model.start, state_dict[token], prob)\n",
    "    \n",
    "    transition_dist_list = list(transition_dist.items())\n",
    "    for i in range(1, len(transition_dist_list)):\n",
    "        ptoken, pmeta = transition_dist_list[i]\n",
    "        if ptoken != '.' and ptoken != '<start>':\n",
    "            for ctoken, cprob in pmeta['probs'].items():\n",
    "                \n",
    "                model.add_transition(\n",
    "                    state_dict[ptoken],\n",
    "                    state_dict[ctoken],\n",
    "                    cprob\n",
    "                )\n",
    "\n",
    "        \n",
    "    return model, state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HmmTaggerModel(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    POS Tagger with Hmm Model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._inner_model = None\n",
    "        self._tag_dist = None\n",
    "        self._transition_dist = None\n",
    "        self._state_dict = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        expecting X as list of tokens, while y is list of POS tag\n",
    "        \"\"\"\n",
    "        combined = list(zip(X, y))\n",
    "        self._tag_dist = construct_discrete_distributions_per_words(combined)\n",
    "        self._transition_dist = construct_transition_probabilities_per_words(combined)\n",
    "        \n",
    "        self._inner_model, _ = build_hmm_model(self._tag_dist, self._transition_dist)\n",
    "        self._inner_model.bake()\n",
    "        \n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        expecting X as list of tokens\n",
    "        \"\"\"\n",
    "        return [state.name for i, state in self._inner_model.viterbi(X)[1]][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ypred, ytrue):\n",
    "    total = len(ytrue)\n",
    "    correct = 0\n",
    "    for pred, true in zip(ypred, ytrue):\n",
    "        if ypred == ytrue:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HmmTaggerModel()\n",
    "model.fit(map(lambda x: x[0], sample), map(lambda x: x[1], sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "st_idx = [0, 68]\n",
    "end_idx = [10, 72]\n",
    "\n",
    "mean_acc = np.mean([\n",
    "    accuracy(\n",
    "        list(map(lambda x: x[1], sample[st:end])),\n",
    "        model.predict(\n",
    "            list(map(lambda x: x[0], sample[st:end]))\n",
    "        )\n",
    "    )    \n",
    "    for st, end in zip(st_idx, end_idx)\n",
    "])\n",
    "\n",
    "print('mean accuracy: {}'.format(mean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_sample = corp.brown.tagged_words()[:500000]\n",
    "model = HmmTaggerModel()\n",
    "model.fit(\n",
    "    map(lambda x: x[0], larger_sample), \n",
    "    map(lambda x: x[1], larger_sample)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "st_idx = [0, 68, 103]\n",
    "end_idx = [10, 72, 118]\n",
    "\n",
    "mean_acc = np.mean([\n",
    "    accuracy(\n",
    "        list(map(lambda x: x[1], sample[st:end])),\n",
    "        model.predict(\n",
    "            list(map(lambda x: x[0], sample[st:end]))\n",
    "        )\n",
    "    )    \n",
    "    for st, end in zip(st_idx, end_idx)\n",
    "])\n",
    "\n",
    "print('mean accuracy: {}'.format(mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- https://en.wikipedia.org/wiki/Brown_Corpus\n",
    "- https://pomegranate.readthedocs.io/en/latest/HiddenMarkovModel.html#hiddenmarkovmodel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
