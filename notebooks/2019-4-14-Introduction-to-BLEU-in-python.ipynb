{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to BLEU in Python\n",
    "\n",
    "**BLEU** is a metric to quantify effectiveness of an Machine Translation (MT). It stands for <strong>B</strong>i<strong>L</strong>ingual <strong>E</strong>valuation <strong>U</strong>nderstudy $^{[1]}$.\n",
    "\n",
    "\n",
    "in general it solves the problem of different human translation **references** by different annotators when comparing to machine generated **translation**. Let's start from using BLEU in NLTK translation package $^{[2]}$\n",
    "\n",
    "Assume we have translation references, generated by 4 annotators:\n",
    "1. this is a ship\n",
    "2. it is ship\n",
    "3. ship it is\n",
    "4. a ship, it is # Yoda style\n",
    "\n",
    "![yoda](https://media.giphy.com/media/3ohuAxV0DfcLTxVh6w/giphy.gif)\n",
    "\n",
    "and our MT system generate:\n",
    "> it is ship\n",
    "\n",
    "\n",
    "\n",
    "Then we asked: how good is this? Given all difference references by human annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# split will transform the sentence into 1-gram\n",
    "reference = [\n",
    "    'this is a ship'.split(),\n",
    "    'it is ship'.split(),\n",
    "    'ship it is'.split(),\n",
    "    'a ship, it is'.split() # master Yoda\n",
    "]\n",
    "\n",
    "translation = 'it is ship'.split()\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it means?\n",
    "\n",
    "the `sentence_bleu` method produces a number from $[0, 1]$ therefore it could be categorized as a good translation.\n",
    "the score $1.0$ means that the machine output has exactly matched one of referenced human translation. Let's take another translation example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.8408964152537145\n"
     ]
    }
   ],
   "source": [
    "translation = 'it is a ship'.split()\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"it is a ship\" in English should also be categorized as a good translation right? However since there are no referenced said exactly like this, it got lower score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of BLEU score\n",
    "\n",
    "Let's again look at these example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.1353352832366127\n",
      "BLEU score: 0.6147881529512643\n",
      "BLEU score: 0.6042750794713536\n",
      "BLEU score: 0.7598356856515925\n",
      "BLEU score: 0.6065306597126334\n"
     ]
    }
   ],
   "source": [
    "translation = 'it'.split()\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))\n",
    "\n",
    "translation = 'it it it it it it it'.split()\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))\n",
    "\n",
    "\n",
    "translation = 'it a b c d e f g h i j k l m n'.split()\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))\n",
    "\n",
    "translation = 'ship ship ship'.split()\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))\n",
    "\n",
    "translation = 'it ship'.split()\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![escalated](https://media.giphy.com/media/5Wi8vPTzWHwXPqCPVN/giphy.gif)\n",
    "\n",
    "these are bad translations, but would result such a high BLEU score! Why is that? Let's go back a little on how to calculate BLEU score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU From Scratch\n",
    "\n",
    "Looking at BLEU paper $^{[1]}$ and Andrew Ng's video $^{[2]}$ we know that we should split our implementations into components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching word vector count with all candidate or one of the candidate is an intuitive and simple way to match translation candidate / reference with hypothesis so we will start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngram(unigram, ngram=1):\n",
    "    \"\"\"\n",
    "    Return\n",
    "    -----\n",
    "    counter: dict, containing ngram as key, and count as value\n",
    "    \"\"\"\n",
    "    return Counter(ngrams(unigram, ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: Counter({('is',): 2, ('this',): 1, ('ship',): 1, ('a',): 1})\n"
     ]
    }
   ],
   "source": [
    "test_unigram = 'this is is a ship'.split()\n",
    "res = count_ngram(test_unigram)\n",
    "\n",
    "assert res[('is',)] == 2\n",
    "assert res[('a',)] == 1\n",
    "\n",
    "print('res: {}'.format(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that we could have several translation candidate for 1 hypothesis. Counting word vector will not be enough as we could have different word vector for each of translation candidate / reference. We wanted to normalize this word vector that we have obtained with word vector that will represent all references that we have.\n",
    "\n",
    "One way to construct word vector from references is to find the **maximum number of ngram occurs in on each reference** instead of summing or averaging. We call this $\\text{Max_Ref_Count}$. Now we want to \"clip\" count for each gram from translation.  \n",
    "\n",
    "$$\n",
    "Count_{clip} = min(Count, \\text{Max_Ref_Count})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clip_ngram(translation_u, list_of_reference_u, ngram=1):\n",
    "    \"\"\"\n",
    "    Return\n",
    "    ----\n",
    "    \n",
    "    \"\"\"\n",
    "    res = dict()\n",
    "    # retrieve hypothesis counts\n",
    "    ct_translation_u = count_ngram(translation_u, ngram=ngram)\n",
    "    \n",
    "    # retrieve translation candidate counts\n",
    "    for reference_u in list_of_reference_u:\n",
    "        ct_reference_u = count_ngram(reference_u, ngram=ngram)\n",
    "        for k in ct_reference_u:\n",
    "            if k in res:\n",
    "                res[k] = max(ct_reference_u[k], res[k])\n",
    "            else:\n",
    "                res[k] = ct_reference_u[k]\n",
    "                \n",
    "\n",
    "    return {\n",
    "        k: min(ct_translation_u.get(k, 0), res.get(k, 0)) \n",
    "        for k in ct_translation_u\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = 'is some'\n",
    "references = [\n",
    "    'this is a test',\n",
    "    'this is is is test'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result from count clip: {('is',): 1, ('some',): 0}\n"
     ]
    }
   ],
   "source": [
    "res = count_clip_ngram(\n",
    "    translation.split(), \n",
    "    list(map(lambda ref: ref.split(), references))\n",
    ")\n",
    "\n",
    "\n",
    "assert res[('is',)] == 1\n",
    "assert res[('some',)] == 0\n",
    "\n",
    "print('result from count clip: {}'.format(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified precission ($p_n$, where $n$ is $n$-gram) is defined by\n",
    "\n",
    "$$\n",
    "p_n = \\frac{\n",
    "    \\sum_{C \\space \\in \\space \\{Candidates\\}} \\sum_{n\\text{-gram} \\space \\in \\space C} Count_{clip}(n\\text{-gram)}\n",
    "}{\n",
    "    \\sum_{C' \\space \\in \\space \\{Candidates\\}} \\sum_{n\\text{-gram}' \\space \\in \\space C'} Count(n\\text{-gram')}\n",
    "}\n",
    "$$\n",
    "\n",
    "or in english language: \n",
    "> sum of all count clipped of n-gram of (translation and references) divided by sum of count n-gram from translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(translation_u, list_of_reference_u, ngram=1):\n",
    "    ct_clip = count_clip_ngram(translation_u, list_of_reference_u, ngram)\n",
    "    ct = count_ngram(translation_u, ngram)\n",
    "    \n",
    "    return sum(ct_clip.values()) / float(max(sum(ct.values()), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brevity Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wanted to show that we will reward translation that matches one of reference candidate length, not shorted and not longer. Since references length may be difference, we could still find and use the **closest/effective reference length**. Here the term Brevity Penalty ($BP$) comes \n",
    "$$\n",
    "BP \\space \\in \\space [0, 1]\n",
    "$$\n",
    "\n",
    "and we make $BP$ function to be\n",
    "\n",
    "$$\n",
    "BP = \n",
    "\\begin{cases} \n",
    "1 & \\text{if} \\space c > r \\\\\n",
    "e^{(1-r/c)} & \\text{if} \\space c \\leq r\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$c$: length of candidate translation\n",
    "\n",
    "$r$: effective reference length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't penalize if $c$ larger than candidate translation because it has already penalized by modified precision. To be exact, because in modified precision when duplicate but relevant word occurs more than it supposed to or when the number of irrelevant word occurs more, it will results in larger denominator thus pulling the $BP$ towards 0.\n",
    "\n",
    "\n",
    "We now want to show implementation of $BP$ in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_ref_length(translation_u, list_of_reference_u):\n",
    "    \"\"\"\n",
    "    determine the closest reference length from translation length\n",
    "    \"\"\"\n",
    "    len_trans = len(translation_u)\n",
    "    closest_ref_idx = np.argmin([abs(len(x) - len_trans) for x in list_of_reference_u])\n",
    "    return len(list_of_reference_u[closest_ref_idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(translation_u, list_of_reference_u):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    c = len(translation_u)\n",
    "    r = closest_ref_length(translation_u, list_of_reference_u)\n",
    "    \n",
    "    if c > r:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.exp(1 - float(r)/c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bleu Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating BLEU after defining steps above is straightforward!\n",
    "\n",
    "\n",
    "$$\n",
    "BLEU = BP.\\exp(\\sum^{N}_{n=1} w_n \\log p_n)\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "$N$: maximum number of $n$-gram. Usually 4.\n",
    "\n",
    "$w_n$: weight for each modified precision. By default use $1/N$, so 0.25 each gram.\n",
    "\n",
    "$p_n$: modified precision for each gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(translation_u, list_of_reference_u, W=[0.25 for x in range(4)]):\n",
    "    bp = brevity_penalty(translation_u, list_of_reference_u)\n",
    "    modified_precisions = [\n",
    "        modified_precision(translation_u, list_of_reference_u, ngram=ngram)\n",
    "        for ngram, _ in enumerate(W,start=1)\n",
    "    ]\n",
    "    score = np.sum([\n",
    "        wn * np.log(modified_precisions[i]) if modified_precisions[i] != 0 else 0 for i, wn in enumerate(W)\n",
    "    ])\n",
    "    \n",
    "    return bp * np.exp(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All is good!\n"
     ]
    }
   ],
   "source": [
    "assert bleu_score(\n",
    "    translation.split(),\n",
    "    list(map(lambda ref: ref.split(), references)),\n",
    ") == bleu.sentence_bleu(\n",
    "    list(map(lambda ref: ref.split(), references)),\n",
    "    translation.split(),\n",
    ")\n",
    "\n",
    "print('All is good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have understand how to construct BLEU score for difference human reference and evaluate a hypothesis translation and we have seen that it did good job in scoring the hypothesis translation with this kind of setting.\n",
    "\n",
    "BLEU might produce relatively good score to bad translation. In one of Phillip Koehn et al$^{[4]}$ work, they have explained phenomenons that makes BLEU flawed. However, it still is a good metric, used to evaluate machine translation performance up until now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] [F. Reznik, “Bleu a Method for Automatic Evaluation of Machine Translation,” Che vuoi ?, vol. 30, no. 2, p. 107, 2015.](https://www.aclweb.org/anthology/P02-1040.pdf)\n",
    "\n",
    "[2] [NLTK: Translation Package](https://www.nltk.org/api/nltk.translate.html)\n",
    "\n",
    "[3] [Andrew Ng's Lecture: BLEU](https://www.youtube.com/watch?v=DejHQYAGb7Q)\n",
    "\n",
    "[4] [C. C. Miles and O. Philipp, “Re-evaluating the Role of Bleu in Machine Translation Research” pp. 249–256, 2002](https://www.aclweb.org/anthology/E06-1032)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
